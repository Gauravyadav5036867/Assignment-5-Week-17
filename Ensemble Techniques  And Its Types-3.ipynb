{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f09df1-a5df-4e53-b04e-e66e01a521cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "A Random forest regression model combines multiple decision trees to create a single model. Each tree in the forest builds from a different subset of the data and makes its own independent prediction. The final prediction for input is based on the average or weighted average of all the individual trees' predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c9dff-e21c-4fc8-8729-f18de60d2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2\n",
    "By incorporating randomness, each tree in the forest exhibits low correlations with others, mitigating the risk of bias. Moreover, the presence of numerous trees helps alleviate overfitting, a situation where the model captures excessive “noise” from the training data, leading to suboptimal decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad17ab6-644d-4f25-a2d2-0fda6287161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 3\n",
    "\n",
    "At certain values of each feature, the decision tree outputs a classification of blue, green, red, etc. The above results are aggregated, through model votes or averaging, into a single ensemble model that ends up outperforming any individual decision tree's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f887c64-d223-4116-a851-e9a29862723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 4\n",
    "\n",
    "This Random Forest hyperparameter specifies the minimum number of samples that should be present in the leaf node after splitting a node. The tree on the left represents an unconstrained tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d7fa2-3d94-4ca3-9aae-6411dd5c61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 5\n",
    "\n",
    "A decision tree is simpler and more interpretable but prone to overfitting, while a random forest is complex and prevents the risk of overfitting. Random forest is more robust and generalized when performing on new data, and it is widely used in various domains such as finance, healthcare, and deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a4eb2-d5c1-4054-998b-51b664dde503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 6\n",
    "\n",
    "Random Forest classifiers offer high predictive precision, flexibility, and immediacy, making them efficient for various applications . However, a significant drawback is their lack of interpretability, as they are considered Black Box models due to the complexity of decision trees within them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbbe14-d452-457b-bdb4-23d4adc8dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 7 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
